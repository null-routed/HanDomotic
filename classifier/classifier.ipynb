{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture Recognition System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "from numpy.fft import rfft, rfftfreq\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import json\n",
    "from scipy.stats import kurtosis, skew, iqr\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Create a list of DataFrames, one for each gesture\n",
    "    dfs = []        \n",
    "    for item in data:\n",
    "        df = pd.DataFrame({\n",
    "            'timestamps': item['timestamps'],\n",
    "            'xTimeSeries': item['xTimeSeries'],\n",
    "            'yTimeSeries': item['yTimeSeries'],\n",
    "            'zTimeSeries': item['zTimeSeries'],\n",
    "            'label': item['label']\n",
    "        })\n",
    "        dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_signal_bandpass(data, lowcut=5.0, highcut=25.0, fs=50, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band', analog=True)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "def extract_features(segment):\n",
    "    features = []\n",
    "    m_axis_data = []  # For M-axis calculation\n",
    "\n",
    "    for axis in ['xTimeSeries', 'yTimeSeries', 'zTimeSeries']:\n",
    "        axis_data = np.array(segment[axis])\n",
    "        m_axis_data.append(axis_data)\n",
    "        \n",
    "        # Time domain features\n",
    "        features += [\n",
    "            np.mean(axis_data),  # Mean\n",
    "            np.std(axis_data),   # Standard Deviation\n",
    "            np.ptp(axis_data),   # Peak to peak (range)\n",
    "            np.min(axis_data),   # Minimum\n",
    "            np.max(axis_data),   # Maximum\n",
    "            np.sum(np.abs(axis_data)) / len(axis_data),  # SMA (for this axis, later average across all axes)\n",
    "\n",
    "        ]\n",
    "\n",
    "        # Average absolute increment\n",
    "        avg_abs_increment = np.mean(np.abs(np.diff(axis_data)))\n",
    "        features.append(avg_abs_increment)\n",
    "\n",
    "        # Mean-crossings (a variant of zero-crossings focusing on mean level)\n",
    "        mean = np.mean(axis_data)\n",
    "        mean_crossings = np.sum(np.diff(axis_data - mean > 0).astype(int))\n",
    "        features.append(mean_crossings)\n",
    "\n",
    "        # Frequency domain features\n",
    "        features += extract_frequency_features(axis_data)\n",
    "\n",
    "    # Pairwise correlations\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 3):\n",
    "            correlation = np.corrcoef(m_axis_data[i], m_axis_data[j])[0, 1]\n",
    "            features.append(correlation)\n",
    "\n",
    "    return features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_frequency_features(data):\n",
    "    fft_vals = np.fft.rfft(data)  # Real FFT\n",
    "    spectral_energy = np.sum(np.abs(fft_vals)**2) / len(fft_vals)\n",
    "    freqs = np.fft.rfftfreq(len(data))\n",
    "    centroid = np.sum(freqs * np.abs(fft_vals)) / np.sum(np.abs(fft_vals))\n",
    "    return [spectral_energy, centroid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(features, labels):\n",
    "    model = make_pipeline(StandardScaler(), SVC(max_iter= 10000))\n",
    "    model.fit(features, labels)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_with_confidence(model, features, threshold = 1):\n",
    "    decision_values = model.decision_function([features])\n",
    "    prediction = model.predict([features])\n",
    "    confidence = np.abs(decision_values[0])  # Use absolute value for confidence\n",
    "    if confidence < threshold:\n",
    "        return \"Other\", confidence  # Return empty label if below threshold\n",
    "    else:\n",
    "        return prediction[0], confidence  # Return prediction and confidence otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1435,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_preprocessed_data(dataframes):\n",
    "    for i, df in enumerate(dataframes):\n",
    "        # Original data\n",
    "        if i < 3:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(df['timestamps'], df['xTimeSeries'], label='Original X', color='red')\n",
    "            plt.plot(df['timestamps'], df['yTimeSeries'], label='Original Y', color='green')\n",
    "            plt.plot(df['timestamps'], df['zTimeSeries'], label='Original Z', color='blue')\n",
    "            plt.title('Original Accelerometer Data')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Acceleration')\n",
    "            plt.legend()\n",
    "\n",
    "        # Preprocessed data\n",
    "        df['xTimeSeries'] = preprocess_signal_bandpass(df['xTimeSeries'])\n",
    "        df['yTimeSeries'] = preprocess_signal_bandpass(df['yTimeSeries'])\n",
    "        df['zTimeSeries'] = preprocess_signal_bandpass(df['zTimeSeries'])\n",
    "        if i < 3:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(df['timestamps'], df['xTimeSeries'], label='Preprocessed X', color='red')\n",
    "            plt.plot(df['timestamps'], df['yTimeSeries'], label='Preprocessed Y', color='green')\n",
    "            plt.plot(df['timestamps'], df['zTimeSeries'], label='Preprocessed Z', color='blue')\n",
    "            plt.title('Preprocessed Accelerometer Data')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Acceleration')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Practice Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1436,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1436], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m labels_test\u001b[38;5;241m.\u001b[39mextend(random_labels)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train the classifier on the training set\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Evaluate the classifier on the testing set (now includes random data)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m predictions_test \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1434], line 2\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(features, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_classifier\u001b[39m(features, labels):\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m make_pipeline(StandardScaler(), \u001b[43mSVC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(features, labels)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'iter'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# Load gesture data\n",
    "\"\"\"\n",
    "dataframes = load_data('labeled_train_data/labeled_data_circle.json')  + load_data('labeled_train_data/labeled_data_clap.json')\n",
    "random_dataframes = load_data('labeled_train_data/labeled_data_random.json')\n",
    "\n",
    "# Plot preprocessed data for visual inspection (if preprocess is defined and required here)\n",
    "# plot_preprocessed_data(dataframes)\n",
    "\n",
    "# Prepare features and labels for gesture data\n",
    "\"\"\"\n",
    "features_file = json.loads(open('features.json', 'r').read())\n",
    "features = [f[\"features\"] for f in features_file if f[\"label\"] != \"Other\"]\n",
    "labels = [l[\"label\"] for l in features_file if l[\"label\"] != \"Other\"]\n",
    "\"\"\"\n",
    "features = []\n",
    "labels = []\n",
    "for df in dataframes:\n",
    "    # Preprocess the data here if necessary, e.g., df['xTimeSeries'] = preprocess_signal(df['xTimeSeries'])\n",
    "    f = extract_features(df)\n",
    "    features.append(f)\n",
    "    labels.append(df['label'].iloc[0])  # Assuming all entries in a df have the same label\n",
    "\"\"\"\n",
    "# Split the gesture data into training and testing sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.20, random_state=42)\n",
    "\n",
    "# Prepare features and labels for random data, and use them only in the testing phase\n",
    "random_features = [f[\"features\"] for f in features_file if f[\"label\"] == \"Other\"]\n",
    "random_labels = [l[\"label\"] for l in features_file if l[\"label\"] == \"Other\"]\n",
    "\"\"\"\n",
    "random_features = []    \n",
    "random_labels = []\n",
    "for df in random_dataframes:\n",
    "    # Preprocess the random data here if necessary\n",
    "    \n",
    "    random_features.append(extract_features(df))\n",
    "    random_labels.append('No Gesture')  # Label all random movements as 'No Gesture'\n",
    "\"\"\"\n",
    "# Combine the test set with the random data\n",
    "features_test.extend(random_features)\n",
    "labels_test.extend(random_labels)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "model = train_classifier(features_train, labels_train)\n",
    "\n",
    "# Evaluate the classifier on the testing set (now includes random data)\n",
    "predictions_test = []\n",
    "confidences_test = []\n",
    "for feature in features_test:\n",
    "    prediction, confidence = predict_with_confidence(model, feature)\n",
    "    predictions_test.append(prediction)\n",
    "    confidences_test.append(confidence)\n",
    "\n",
    "for i, tup in enumerate(zip(predictions_test, labels_test)):\n",
    "    print(f\"Predicted: {tup[0]}, Actual: {tup[1]}, with confidence: {confidences_test[i]:.2f}\")\n",
    "\n",
    "# Calculate accuracy on the testing data including random data\n",
    "accuracy_test = sum(1 for p, l in zip(predictions_test, labels_test) if p == l) / len(labels_test)\n",
    "print(f\"Accuracy on test data including random movements: {accuracy_test:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "feature_count = len(features_train[0])  # Replace this with the actual number of features if known\n",
    "initial_type = [('float_input', FloatTensorType([None, feature_count]))]\n",
    "\n",
    "onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "with open(\"gesture_classifier.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Circle, Actual: Circle, with confidence: 1.16\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.07\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.05\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.12\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.21\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.18\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.12\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.00\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.10\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.14\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.12\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.33\n",
      "Predicted: Other, Actual: Clap, with confidence: 0.98\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.18\n",
      "Predicted: Other, Actual: Clap, with confidence: 0.89\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.26\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.01\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.35\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.15\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.08\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.08\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.27\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.32\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.04\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.22\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.20\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.01\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.24\n",
      "Predicted: Clap, Actual: Clap, with confidence: 1.29\n",
      "Predicted: Circle, Actual: Circle, with confidence: 1.26\n",
      "Predicted: Other, Actual: Other, with confidence: 0.61\n",
      "Predicted: Other, Actual: Other, with confidence: 0.70\n",
      "Predicted: Other, Actual: Other, with confidence: 0.73\n",
      "Predicted: Other, Actual: Other, with confidence: 0.52\n",
      "Predicted: Other, Actual: Other, with confidence: 0.51\n",
      "Predicted: Other, Actual: Other, with confidence: 0.53\n",
      "Predicted: Other, Actual: Other, with confidence: 0.46\n",
      "Predicted: Other, Actual: Other, with confidence: 0.54\n",
      "Predicted: Other, Actual: Other, with confidence: 0.63\n",
      "Predicted: Other, Actual: Other, with confidence: 0.47\n",
      "Predicted: Other, Actual: Other, with confidence: 0.61\n",
      "Predicted: Other, Actual: Other, with confidence: 0.57\n",
      "Predicted: Other, Actual: Other, with confidence: 0.37\n",
      "Predicted: Other, Actual: Other, with confidence: 0.00\n",
      "Predicted: Other, Actual: Other, with confidence: 0.43\n",
      "Predicted: Other, Actual: Other, with confidence: 0.38\n",
      "Predicted: Other, Actual: Other, with confidence: 0.15\n",
      "Predicted: Other, Actual: Other, with confidence: 0.57\n",
      "Predicted: Other, Actual: Other, with confidence: 0.60\n",
      "Predicted: Other, Actual: Other, with confidence: 0.15\n",
      "Predicted: Other, Actual: Other, with confidence: 0.28\n",
      "Predicted: Other, Actual: Other, with confidence: 0.32\n",
      "Predicted: Other, Actual: Other, with confidence: 0.13\n",
      "Predicted: Other, Actual: Other, with confidence: 0.47\n",
      "Predicted: Other, Actual: Other, with confidence: 0.18\n",
      "Predicted: Other, Actual: Other, with confidence: 0.55\n",
      "Predicted: Other, Actual: Other, with confidence: 0.55\n",
      "Predicted: Other, Actual: Other, with confidence: 0.68\n",
      "Predicted: Other, Actual: Other, with confidence: 0.37\n",
      "Predicted: Other, Actual: Other, with confidence: 0.01\n",
      "Predicted: Other, Actual: Other, with confidence: 0.40\n",
      "Predicted: Other, Actual: Other, with confidence: 0.51\n",
      "Predicted: Other, Actual: Other, with confidence: 0.49\n",
      "Predicted: Other, Actual: Other, with confidence: 0.44\n",
      "Predicted: Other, Actual: Other, with confidence: 0.56\n",
      "Predicted: Other, Actual: Other, with confidence: 0.21\n",
      "Predicted: Other, Actual: Other, with confidence: 0.46\n",
      "Predicted: Other, Actual: Other, with confidence: 0.99\n",
      "Predicted: Other, Actual: Other, with confidence: 0.65\n",
      "Predicted: Other, Actual: Other, with confidence: 0.16\n",
      "Predicted: Other, Actual: Other, with confidence: 0.02\n",
      "Predicted: Other, Actual: Other, with confidence: 0.08\n",
      "Predicted: Other, Actual: Other, with confidence: 0.33\n",
      "Predicted: Other, Actual: Other, with confidence: 0.18\n",
      "Predicted: Other, Actual: Other, with confidence: 0.13\n",
      "Predicted: Other, Actual: Other, with confidence: 0.45\n",
      "Predicted: Other, Actual: Other, with confidence: 0.36\n",
      "Predicted: Other, Actual: Other, with confidence: 0.16\n",
      "Predicted: Other, Actual: Other, with confidence: 0.54\n",
      "Predicted: Other, Actual: Other, with confidence: 0.00\n",
      "Predicted: Other, Actual: Other, with confidence: 0.46\n",
      "Predicted: Other, Actual: Other, with confidence: 0.70\n",
      "Predicted: Other, Actual: Other, with confidence: 0.56\n",
      "Predicted: Other, Actual: Other, with confidence: 0.65\n",
      "Predicted: Other, Actual: Other, with confidence: 0.63\n",
      "Predicted: Other, Actual: Other, with confidence: 0.86\n",
      "Predicted: Other, Actual: Other, with confidence: 0.77\n",
      "Predicted: Other, Actual: Other, with confidence: 0.56\n",
      "Predicted: Other, Actual: Other, with confidence: 0.65\n",
      "Predicted: Other, Actual: Other, with confidence: 0.75\n",
      "Predicted: Other, Actual: Other, with confidence: 0.45\n",
      "Predicted: Other, Actual: Other, with confidence: 0.62\n",
      "Predicted: Other, Actual: Other, with confidence: 0.60\n",
      "Predicted: Other, Actual: Other, with confidence: 0.22\n",
      "Predicted: Other, Actual: Other, with confidence: 0.17\n",
      "Predicted: Other, Actual: Other, with confidence: 0.59\n",
      "Predicted: Other, Actual: Other, with confidence: 0.61\n",
      "Predicted: Other, Actual: Other, with confidence: 0.65\n",
      "Predicted: Other, Actual: Other, with confidence: 0.65\n",
      "Predicted: Other, Actual: Other, with confidence: 0.72\n",
      "Predicted: Other, Actual: Other, with confidence: 0.25\n",
      "Predicted: Other, Actual: Other, with confidence: 0.09\n",
      "Predicted: Other, Actual: Other, with confidence: 0.36\n",
      "Predicted: Other, Actual: Other, with confidence: 0.85\n",
      "Predicted: Other, Actual: Other, with confidence: 0.45\n",
      "Predicted: Other, Actual: Other, with confidence: 0.60\n",
      "Predicted: Other, Actual: Other, with confidence: 0.63\n",
      "Predicted: Other, Actual: Other, with confidence: 0.48\n",
      "Predicted: Other, Actual: Other, with confidence: 0.65\n",
      "Predicted: Other, Actual: Other, with confidence: 0.50\n",
      "Predicted: Other, Actual: Other, with confidence: 0.74\n",
      "Predicted: Other, Actual: Other, with confidence: 0.66\n",
      "Predicted: Other, Actual: Other, with confidence: 0.54\n",
      "Predicted: Other, Actual: Other, with confidence: 0.68\n",
      "Accuracy on test data including random movements: 0.98 and average confidence: 0.65\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "\n",
    "def onnx_predict_with_confidence(onnx_model_path, features, threshold=1):\n",
    "    # Load the ONNX model\n",
    "    sess = rt.InferenceSession(onnx_model_path)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_label = sess.get_outputs()[0].name  # The predicted label output\n",
    "    output_probability = sess.get_outputs()[1].name  # The probabilities output\n",
    "\n",
    "    # Run the model (ensure features is a numpy array and correctly shaped)\n",
    "    probabilities = sess.run([output_probability], {input_name: np.array([features], dtype=np.float32)})[0]\n",
    "    predictions = sess.run([output_label], {input_name: np.array([features], dtype=np.float32)})[0]\n",
    "    \n",
    "    # Calculate confidence (the highest class probability)\n",
    "    max_prob = np.max(probabilities)\n",
    "    prediction = predictions[0]\n",
    "\n",
    "    if max_prob < threshold:\n",
    "        return \"Other\", max_prob\n",
    "    else:\n",
    "        return prediction, max_prob\n",
    "\n",
    "# Example usage\n",
    "predictions_test = []\n",
    "confidences_test = []\n",
    "for feature in features_test:\n",
    "    prediction, confidence = onnx_predict_with_confidence(\"gesture_classifier.onnx\",feature)\n",
    "    predictions_test.append(prediction)\n",
    "    confidences_test.append(confidence)\n",
    "    \n",
    "conf_total = 0\n",
    "for i, tup in enumerate(zip(predictions_test, labels_test)):\n",
    "    print(f\"Predicted: {tup[0]}, Actual: {tup[1]}, with confidence: {confidences_test[i]:.2f}\")\n",
    "    conf_total += confidences_test[i]\n",
    "\n",
    "# Calculate accuracy on the testing data including random data\n",
    "accuracy_test = sum(1 for p, l in zip(predictions_test, labels_test) if p == l) / len(labels_test)\n",
    "avg_conf = conf_total / len(labels_test)\n",
    "print(f\"Accuracy on test data including random movements: {accuracy_test:.2f} and average confidence: {avg_conf:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
